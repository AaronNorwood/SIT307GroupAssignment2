{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>SIT307 Group Assignment 1 notebook</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Group 3</h3>\n",
    "<p>By:<br>\n",
    "    Aaron Norwood,218330434<br>\n",
    "    Joshua Anthony, 219466473<br>\n",
    "    Roger Middenway, 217602784<br>\n",
    "    David Adams, 216110104<br>\n",
    "    Linden Hutchinson, 218384326<br>\n",
    "    Dale Orders, 219106283"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imported libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import math\n",
    "from scipy import stats\n",
    "import plotly.express as px\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Read in the data, store if dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./data/healthcare-dataset-stroke-data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "<h4>Tidying"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## drop unnecessary id column\n",
    "df.drop(['id'], axis=1, inplace=True)\n",
    "\n",
    "##convert gender to lowercase\n",
    "df['gender'] = df['gender'].apply(lambda x: x.lower())\n",
    "\n",
    "## drop the one row that has \"other\" gender to keep things simple\n",
    "df.drop(index=df[df['gender'] == 'other'].index, inplace=True)\n",
    "\n",
    "# replace gender with binary values\n",
    "df['gender'] = df['gender'].str.lower().map({'male': 1, 'female': 0})\n",
    "\n",
    "# replace marital status with binary values\n",
    "df['ever_married'] = df['ever_married'].str.lower().map({'yes': 1, 'no': 0})\n",
    "\n",
    "##convert work_type to lowercase ensure consistent spacing \n",
    "df['work_type'] = df['work_type'].apply(lambda x: x.lower().replace('_','-'))\n",
    "\n",
    "##convert residence_type to lowercase\n",
    "df.rename(columns={'Residence_type':'residence_type'}, inplace=True)\n",
    "df['residence_type'] = df['residence_type'].apply(lambda x: x.lower())\n",
    "\n",
    "##convert smoking_status to lowercase ensure consistent spacing \n",
    "df['smoking_status'] = df['smoking_status'].apply(lambda x: x.lower().replace(' ', '-'))\n",
    "\n",
    "## round off age\n",
    "df['age'] = df['age'].apply(lambda x : round(x))\n",
    "\n",
    "# cap BMI outliers to a maximum of 60 \n",
    "df['bmi'] = df['bmi'].apply(lambda bmi_value: bmi_value if 12 < bmi_value < 60 else np.nan)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Imputing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "m_df = df[df['gender'] == 1]\n",
    "f_df = df[df['gender'] == 0]\n",
    "\n",
    "m_bmi_avg = m_df.groupby('age')['bmi'].mean()\n",
    "f_bmi_avg = f_df.groupby('age')['bmi'].mean()\n",
    "##round to one to fit with other bmi values\n",
    "m_bmi_avg = round(m_bmi_avg,1)\n",
    "f_bmi_avg = round(f_bmi_avg,1)\n",
    "\n",
    "missing_vals = df[df.isnull().any(axis = 1)]\n",
    "\n",
    "for index, row in missing_vals.iterrows():\n",
    "    if row['gender'] == 1:\n",
    "        df.loc[index,['bmi']] = m_bmi_avg[row['age']]\n",
    "    else:\n",
    "        df.loc[index,['bmi']] = f_bmi_avg[row['age']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Binning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# implement age bins\n",
    "df['age_bin'] = pd.qcut(df['age'], q = 10, precision=1)\n",
    "\n",
    "# implement glucose bins\n",
    "df['avg_glucose_level_bin'] = pd.qcut(df['avg_glucose_level'], q=10, precision=1)\n",
    "\n",
    "# implement bmi bins\n",
    "df['bmi_bin'] = pd.qcut(df['bmi'], q=10, precision=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Add Dummies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_dummy(target_df, origin_df, col):\n",
    "    abbrev = col[:2] + '_'\n",
    "    for cat in origin_df[col].value_counts().index.tolist()[1:]:\n",
    "        target_df[abbrev + str(cat)] = origin_df[col] == cat\n",
    "        target_df[abbrev + str(cat)] = target_df[abbrev + str(cat)].map({False: 0, True: 1})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dummies = pd.DataFrame(index=df.index)\n",
    "\n",
    "add_dummy(df_dummies, df, 'work_type')\n",
    "add_dummy(df_dummies, df, 'smoking_status')\n",
    "add_dummy(df_dummies, df, 'age_bin')\n",
    "add_dummy(df_dummies, df, 'avg_glucose_level_bin')\n",
    "add_dummy(df_dummies, df, 'bmi_bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_add = ['gender', 'hypertension', 'heart_disease', 'ever_married', 'residence_type', 'stroke']\n",
    "df_dummies[to_add] = df[to_add]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected = ['av_(80.0, 85.6]',\n",
    "'wo_self-employed',\n",
    "'sm_formerly-smoked',\n",
    "'ag_(38.0, 45.0]',\n",
    "'ever_married',\n",
    "'ag_(11.0, 20.0]',\n",
    "'ag_(30.0, 38.0]',\n",
    "'ag_(-0.1, 11.0]',\n",
    "'ag_(20.0, 30.0]',\n",
    "'bm_(29.9, 31.8]',\n",
    "'wo_children',\n",
    "'av_(192.2, 271.7]',\n",
    "'hypertension',\n",
    "'ag_(65.0, 75.0]',\n",
    "'heart_disease',\n",
    "'ag_(75.0, 82.0]',\n",
    " 'stroke']\n",
    "\n",
    "df_sel = df_dummies[selected]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import prince\n",
    "mca_cols = ['gender', 'hypertension', 'heart_disease', 'ever_married', 'work_type', 'residence_type', 'smoking_status','age_bin', 'avg_glucose_level_bin', 'bmi_bin']\n",
    "df_mca = df[mca_cols]\n",
    "\n",
    "mca = prince.MCA()\n",
    "mca.fit(df_mca)\n",
    "mca.transform(df_mca)\n",
    "\n",
    "df_mixed_mca = pd.merge(df_sel, mca.row_coordinates(df_mca), left_index=True, right_index=True)\n",
    "\n",
    "X = df_mixed_mca.drop(['stroke'], axis=1)\n",
    "y = df_mixed_mca['stroke']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report, plot_confusion_matrix, roc_auc_score, precision_score, recall_score, f1_score\n",
    "from sklearn.svm import SVC\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, KFold, RepeatedStratifiedKFold, cross_val_score\n",
    "from IPython.display import clear_output\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_random_float(min, max):\n",
    "    return min + (max-min)*random.random()\n",
    "\n",
    "def get_svc(class_weight=None, kernel=\"rbf\", gamma='scale', shrinking=True, probability=False, break_ties=False):\n",
    "    return SVC(class_weight=class_weight, kernel=kernel, gamma=gamma,shrinking=shrinking,probability=False)\n",
    "\n",
    "def get_cv_scores(model, X, y, n_splits=5,n_repeats=10, scoring=None):\n",
    "    cv = RepeatedStratifiedKFold(n_splits=n_splits, n_repeats=n_repeats, random_state=1)\n",
    "    return cross_val_score(model, X, y, scoring=scoring, cv=cv, n_jobs=-1)\n",
    "\n",
    "def run_model(model, X, y, metric=\"recall\"):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=82)\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    plot_confusion_matrix(model, X_test, y_test)\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    \n",
    "def get_metrics_score(model, X, y):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=82, stratify=y)\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    scores = {\n",
    "        'recall':recall_score(y_test, y_pred, labels=np.unique(y_pred), zero_division=0),\n",
    "        'f1':f1_score(y_test, y_pred, labels=np.unique(y_pred), zero_division=0),\n",
    "        'roc_auc':roc_auc_score(y_test, y_pred, labels=np.unique(y_pred)),\n",
    "        'precision':precision_score(y_test, y_pred, labels=np.unique(y_pred), zero_division=0),\n",
    "    }\n",
    "    return scores\n",
    "\n",
    "def get_reports_score(model, X, y):\n",
    "    '''\n",
    "    returns the recall, f1 and precision values obtained for the \"1\" class in a classification report\n",
    "    '''\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=82)\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    report = classification_report(y_test, y_pred, output_dict=True)\n",
    "    \n",
    "    scores = {\n",
    "        'recall':report['1']['recall'],\n",
    "        'f1':report['1']['f1-score'],\n",
    "        'precision':report['1']['precision'],\n",
    "    }\n",
    "    return scores\n",
    "\n",
    "def check_if_better(old, new, thres=1.2):\n",
    "    '''\n",
    "    divides the new value by the old value and checks to see if this is greater than a threshold\n",
    "    if new/old > 1, the new value is better than the old\n",
    "    if new/old < 1, the new value is worse\n",
    "    By setting the threshold, you can decide how much worse than the old is acceptable for the new value\n",
    "    '''\n",
    "    \n",
    "    if old == 0:\n",
    "        return True\n",
    "    if(new/old > thres):\n",
    "        return True\n",
    "    \n",
    "    return False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grid Search for optimizing model metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall Score: 0.040 - best recall score is 0.600\n",
      "w1 3.7\n",
      "w2 34.9\n",
      "gamma scale\n",
      "kernel sigmoid\n",
      "probability 1\n",
      "shrinking 1\n",
      "recall 0.6\n",
      "f1 0.21276595744680854\n",
      "roc_auc 0.6960905349794239\n",
      "precision 0.12931034482758622\n",
      "Total complete: 100.00%\n"
     ]
    }
   ],
   "source": [
    "# this was much more convoluted previously, which is why I bothered to output the remaining percentage\n",
    "# as I narrowed in on the optimum parameters, I could remove loops entirely and speed up the process alot\n",
    "\n",
    "weights = [round(get_random_float(0, 50), 1) for i in range(10)]\n",
    "kernel = ['linear', 'poly', 'rbf', 'sigmoid']\n",
    "gamma = ['scale','auto']\n",
    "shrinking = [1, 0]\n",
    "probability = [1,0]\n",
    "\n",
    "best = {\n",
    "    'recall':0,\n",
    "    'precision':0,\n",
    "    'roc':0,\n",
    "    'f1':0\n",
    "}\n",
    "\n",
    "best_cell = {}\n",
    "cells = []\n",
    "total_len = len(weights)*len(weights)*len(gamma)*len(kernel)*len(shrinking)*len(probability)\n",
    "counter = 0\n",
    "for w1 in weights:\n",
    "    for w2 in weights[::-1]:\n",
    "        for k in kernel:\n",
    "            for g in gamma:\n",
    "                for s in shrinking:\n",
    "                    for p in probability:\n",
    "                \n",
    "                        counter +=1\n",
    "\n",
    "                        # while testing it was found that the actual value of the weights didn't seem to matter, the important thing was the proportion of w2 to w1\n",
    "                        # proportions of between 2 and 3:1 gave the best all round results for metrics\n",
    "                        # As the proportion increases, False Negatives slightly decrease while False Positives increase significantly\n",
    "                        # a proportion of 20:1 will maximize recall but to the detriment of other metrics\n",
    "\n",
    "                        class_weight = {0: w1, 1: w2}\n",
    "                        svc = get_svc(class_weight = class_weight, kernel=k, gamma=g, shrinking=s, probability=p) \n",
    "                        scores = get_metrics_score(svc, X, y)\n",
    "\n",
    "                        cell = {\n",
    "                            'w1':w1,\n",
    "                            'w2':w2,\n",
    "                            'gamma':g,\n",
    "                            'kernel':k,\n",
    "                            'probability':p,\n",
    "                            'shrinking':s,\n",
    "                            'recall':scores['recall'],\n",
    "                            'f1':scores['f1'],\n",
    "                            'roc_auc':scores['roc_auc'],\n",
    "                            'precision':scores['precision'],\n",
    "\n",
    "                        }\n",
    "                        clear_output(wait=True)\n",
    "\n",
    "                        # only update the best cell if\n",
    "                        # recall is better than the current best\n",
    "                        # roc_auc isn't more than 15% worse than the current best\n",
    "                        # precision isn't more than 15% worse than the current best\n",
    "                        # f1 isn't more than 10% worse than the current best\n",
    "                        if scores['recall'] > best['recall'] and check_if_better(best['roc'], scores['roc_auc'], 0.7) and check_if_better(best['precision'], scores['precision'], 0.7) and check_if_better(best['f1'], scores['f1'], 0.7):\n",
    "                            best_cell = cell\n",
    "                            best['recall'] = scores['recall']\n",
    "                            best['f1'] = scores['f1']\n",
    "                            best['roc'] = scores['roc_auc']\n",
    "                            best['precision'] = scores['precision']\n",
    "                            print(f\"Recall score: {scores['recall']:.3f} - new best\")\n",
    "                        else:\n",
    "                            print(f\"Recall Score: {scores['recall']:.3f} - best recall score is {best['recall']:.3f}\")\n",
    "                        [print(key, value, end=\"\\n\") for key, value in best_cell.items()]\n",
    "                        print(f\"Total complete: {100*(counter/total_len):.2f}%\")\n",
    "                        cells.append(cell)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "an integer is required",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-16-14d48b08b60d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m# cv_scores = get_cv_scores(model, X, y)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;31m# print(\"Mean cv score\", np.mean(cv_scores))\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[0mrun_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-12-e6959b1fcb08>\u001b[0m in \u001b[0;36mrun_model\u001b[1;34m(model, X, y, metric)\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mrun_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmetric\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"recall\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m     \u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.33\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m82\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m     \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m     \u001b[0my_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m     \u001b[0mplot_confusion_matrix\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\linde\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\svm\\_base.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    224\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    225\u001b[0m         \u001b[0mseed\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrnd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miinfo\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'i'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 226\u001b[1;33m         \u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msolver_type\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkernel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrandom_seed\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mseed\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    227\u001b[0m         \u001b[1;31m# see comment on the other call to np.iinfo in this file\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    228\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\linde\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\svm\\_base.py\u001b[0m in \u001b[0;36m_dense_fit\u001b[1;34m(self, X, y, sample_weight, solver_type, kernel, random_seed)\u001b[0m\n\u001b[0;32m    283\u001b[0m                 \u001b[0mcache_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcache_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcoef0\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcoef0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    284\u001b[0m                 \u001b[0mgamma\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_gamma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepsilon\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mepsilon\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 285\u001b[1;33m                 max_iter=self.max_iter, random_seed=random_seed)\n\u001b[0m\u001b[0;32m    286\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    287\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_warn_from_fit_status\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32msklearn\\svm\\_libsvm.pyx\u001b[0m in \u001b[0;36msklearn.svm._libsvm.fit\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: an integer is required"
     ]
    }
   ],
   "source": [
    "weight = {0: best_cell['w1'], 1: best_cell['w2']}\n",
    "          \n",
    "model = get_svc(weight, best_cell['kernel'], best_cell['gamma'], shrinking=shrinking, probability=probability)\n",
    "\n",
    "# cv_scores = get_cv_scores(model, X, y)\n",
    "# print(\"Mean cv score\", np.mean(cv_scores))\n",
    "run_model(model, X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Static Optimized SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight = {0: 1, 1: 3}\n",
    "kernel = \"linear\"\n",
    "gamma = \"auto\"\n",
    "shrinking = True\n",
    "probability = True\n",
    "model = get_svc(weight, kernel, gamma, shrinking=shrinking, probability=probability)\n",
    "\n",
    "cv_scores = get_cv_scores(model, X, y)\n",
    "print(\"Mean cv score\", np.mean(cv_scores))\n",
    "run_model(model, X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
